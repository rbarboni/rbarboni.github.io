---
layout: post
title:  "New preprint"
date:   2025-07-23 11:58:50 +0100
---

We published a new preprint on ["Ultra-fast feature learning for the training of two-layer neural networks in the two-timescale regime"](https://arxiv.org/abs/2504.18208).

Considering a *Variable Projection* [[1]](#VarPro) or *two-timescale learning* [[2]](#TwoTimeScale) strategy, we show that during training, the distribution of inner weights of two-layer neural networks evolve according to an *ultra-fast diffusion equation* [[3]](#UltraFast).

![](../files/diffusion_animation.gif)

#### References

<a id="VarPro">[1]</a> 
G. H. Golub, V. Pereyra.
The differentiation of pseudo-inverses and nonlinear least squares problems whose variables separate. 
*SIAM Journal on numerical analysis* (1973).

<a id="TwoTimeScale">[2]</a> 
P. Marion, R. Berthier.
Leveraging the two-timescale regime to demonstrate convergence of neural networks.
*Advances in Neural Information Processing System* (2023).

<a id="UltraFast">[3]</a>
M. Iacobelli, F. S. Patacchini, F. Santambrogio.
Weighted ultrafast diffusion equations: from well-posedness to long-time behaviour.
*Archive for Rational Mechanics and Analysis* (2019).
